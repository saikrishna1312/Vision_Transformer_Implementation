{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "WYRvyx3Rf0Ni"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l8J1DCIcfo9w"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (batch_size, embed_dim, num_patches, num_patches)\n",
        "        x = x.flatten(2)  # Flatten (batch_size, embed_dim, num_patches)\n",
        "        x = x.transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n",
        "        return x\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention needs (sequence_length, batch_size, embed_dim)\n",
        "        x = x.transpose(0, 1)  # (num_patches, batch_size, embed_dim)\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        return attn_output.transpose(0, 1)  # (batch_size, num_patches, embed_dim)\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # MHSA block\n",
        "        attn_output = self.mhsa(x)\n",
        "        x = self.norm1(x + self.dropout(attn_output))  # Add & Norm\n",
        "\n",
        "        # FFN block\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))  # Add & Norm\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=1000,\n",
        "                 embed_dim=768, num_heads=12, depth=12, hidden_dim=3072):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "\n",
        "        # Patch Embedding\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        self.num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Class token and Positional Embedding\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
        "        self.pos_dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, hidden_dim)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Classification Head\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Patch + Position Embedding\n",
        "        x = self.patch_embed(x)  # (batch_size, num_patches, embed_dim)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (batch_size, num_patches + 1, embed_dim)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_dropout(x)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Classification Head\n",
        "        cls_token_final = x[:, 0]  # Take only the CLS token for classification\n",
        "        out = self.mlp_head(cls_token_final)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Instantiate the model\n",
        "# vit = VisionTransformer(img_size=224, patch_size=16, num_classes=10)\n",
        "\n",
        "# # Test the model with a dummy input\n",
        "# dummy_input = torch.randn(1, 3, 224, 224)  # (batch_size, channels, height, width)\n",
        "# output = vit(dummy_input)\n",
        "# print(output.shape)  # Expected output: (1, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrcEI1p5f_cP",
        "outputId": "616c46ef-12ab-4a50-de1d-7d59543f7991"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Transformations (Resizing CIFAR-10 to fit ViT's expected input size)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224 as required by ViT\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Download and load the CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JT7hSvphX_C",
        "outputId": "8c5dd2ee-806e-42dd-eb1c-a180a95dc0df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 42.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the ViT model and move it to the device\n",
        "vit_model = VisionTransformer(img_size=224, patch_size=16, num_classes=10).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vit_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "def train_vit(model, train_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Print stats after each epoch\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Train the model for 5 epochs\n",
        "train_vit(vit_model, train_loader, criterion, optimizer, device, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR7uLJN3gJGL",
        "outputId": "bdab5c16-5fd5-4340-9732-dd1b18179128"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 2.3619, Accuracy: 10.14%\n",
            "Epoch [2/5], Loss: 2.3085, Accuracy: 10.07%\n",
            "Epoch [3/5], Loss: 2.3047, Accuracy: 10.10%\n",
            "Epoch [4/5], Loss: 2.3049, Accuracy: 10.17%\n",
            "Epoch [5/5], Loss: 2.3044, Accuracy: 10.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_vit(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_vit(vit_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chQbY3qAg6tI",
        "outputId": "aaa89733-21e8-4469-890f-7ce50a3cc386"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 10.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hp7uWd39D_A8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}